# DQN_Series

1.DQN :
Mnih, Volodymyr, et al. "Playing atari with deep reinforcement learning." arXiv preprint arXiv:1312.5602 (2013).
https://arxiv.org/abs/1312.5602

2.Double DQN :
Van Hasselt, Hado, Arthur Guez, and David Silver. "Deep reinforcement learning with double q-learning." arXiv preprint arXiv:1509.06461 (2015).
https://arxiv.org/abs/1509.06461

3.Duelling PER :
Wang, Ziyu, et al. "Dueling network architectures for deep reinforcement learning." International conference on machine learning. 2016.
https://arxiv.org/abs/1509.06461

4.PER :
Schaul, Tom, et al. "Prioritized experience replay." arXiv preprint arXiv:1511.05952 (2015).
https://arxiv.org/abs/1511.05952

# Actor-Critic Series

1.DDPG :
Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971 (2015).
https://arxiv.org/abs/1509.02971

2.SAC :
Haarnoja, Tuomas, et al. "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor." arXiv preprint arXiv:1801.01290 (2018).
https://arxiv.org/abs/1801.01290

3.SAC-v2 :
Haarnoja, Tuomas, et al. "Soft actor-critic algorithms and applications." arXiv preprint arXiv:1812.05905 (2018).
https://arxiv.org/abs/1812.05905

4.TD3 :
Fujimoto, Scott, Herke Van Hoof, and David Meger. "Addressing function approximation error in actor-critic methods." arXiv preprint arXiv:1802.09477 (2018).
https://arxiv.org/pdf/1802.09477.pdf

5.MA-TD3 ( for own exprience ) :
No reference paper
